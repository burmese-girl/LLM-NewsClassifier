{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eccf4f46-17f1-4283-ad77-8ba064877d4f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    TITLE CATEGORY\n",
      "0       Fed official says weak data caused by weather,...        b\n",
      "1       Fed's Charles Plosser sees high bar for change...        b\n",
      "2       US open: Stocks fall after Fed official hints ...        b\n",
      "3       Fed risks falling 'behind the curve', Charles ...        b\n",
      "4       Fed's Plosser: Nasty Weather Has Curbed Job Gr...        b\n",
      "...                                                   ...      ...\n",
      "422414  Surgeons to remove 4-year-old's rib to rebuild...        m\n",
      "422415  Boy to have surgery on esophagus after battery...        m\n",
      "422416  Child who swallowed battery to have reconstruc...        m\n",
      "422417  Phoenix boy undergoes surgery to repair throat...        m\n",
      "422418  Phoenix boy undergoes surgery to repair throat...        m\n",
      "\n",
      "[422419 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from tqdm import tqdm\n",
    "import argparse\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "\n",
    "# s3_path = 'https://testing-may-uci-dataset.s3.us-east-1.amazonaws.com/newsCorpora.csv'\n",
    "s3_path = '/home/mayyi/Downloads/news+aggregator/newsCorpora.csv'\n",
    "df = pd.read_csv(s3_path, sep='\\t',names=['ID', 'TITLE', 'URL', 'PUBLISHER', 'CATEGORY', 'STORY', 'HOSTNAME', 'TIMESTAMP'])\n",
    "df = df[['TITLE', 'CATEGORY']]\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8cc55622-73e3-4c95-b8cc-025165e0c5e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    TITLE  CATEGORY\n",
      "0       Fed official says weak data caused by weather,...  Business\n",
      "1       Fed's Charles Plosser sees high bar for change...  Business\n",
      "2       US open: Stocks fall after Fed official hints ...  Business\n",
      "3       Fed risks falling 'behind the curve', Charles ...  Business\n",
      "4       Fed's Plosser: Nasty Weather Has Curbed Job Gr...  Business\n",
      "...                                                   ...       ...\n",
      "422414  Surgeons to remove 4-year-old's rib to rebuild...    Health\n",
      "422415  Boy to have surgery on esophagus after battery...    Health\n",
      "422416  Child who swallowed battery to have reconstruc...    Health\n",
      "422417  Phoenix boy undergoes surgery to repair throat...    Health\n",
      "422418  Phoenix boy undergoes surgery to repair throat...    Health\n",
      "\n",
      "[422419 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "my_dict = {\n",
    "    'e':'Entertainment',\n",
    "    'b':'Business',\n",
    "    't':'Science',\n",
    "    'm':'Health'\n",
    "          }\n",
    "\n",
    "def update_categ(x):\n",
    "    return my_dict[x]\n",
    "\n",
    "df['CATEGORY'] = df['CATEGORY'].apply(lambda x: update_categ(x))\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9298022b-687e-4602-a0a0-65d2a5e18e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               TITLE       CATEGORY\n",
      "0  N.C. Senate wants to tighten proposed coal ash...       Business\n",
      "1  Texas Tech Cheerleader Kendall Jones Is Big Ga...  Entertainment\n",
      "2  Thinner screen, tougher requirements? Why Appl...        Science\n"
     ]
    }
   ],
   "source": [
    "#df = df.sample(frac=0.05,random_state=1)\n",
    "#df = df.reset_index(drop=True)\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "22c5f52e-f7ab-4bb9-9c86-0b1175cf8043",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    TITLE  CATEGORY  \\\n",
      "0       Fed official says weak data caused by weather,...  Business   \n",
      "1       Fed's Charles Plosser sees high bar for change...  Business   \n",
      "2       US open: Stocks fall after Fed official hints ...  Business   \n",
      "3       Fed risks falling 'behind the curve', Charles ...  Business   \n",
      "4       Fed's Plosser: Nasty Weather Has Curbed Job Gr...  Business   \n",
      "...                                                   ...       ...   \n",
      "422414  Surgeons to remove 4-year-old's rib to rebuild...    Health   \n",
      "422415  Boy to have surgery on esophagus after battery...    Health   \n",
      "422416  Child who swallowed battery to have reconstruc...    Health   \n",
      "422417  Phoenix boy undergoes surgery to repair throat...    Health   \n",
      "422418  Phoenix boy undergoes surgery to repair throat...    Health   \n",
      "\n",
      "        ENCODE_CAT  \n",
      "0                0  \n",
      "1                0  \n",
      "2                0  \n",
      "3                0  \n",
      "4                0  \n",
      "...            ...  \n",
      "422414           3  \n",
      "422415           3  \n",
      "422416           3  \n",
      "422417           3  \n",
      "422418           3  \n",
      "\n",
      "[422419 rows x 3 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "encode_dict = {}\n",
    "\n",
    "def encode_cat(x):\n",
    "    if x not in encode_dict.keys():\n",
    "        encode_dict[x]=len(encode_dict)\n",
    "    return encode_dict[x]\n",
    "\n",
    "df['ENCODE_CAT']= df['CATEGORY'].apply(lambda x:encode_cat(x))\n",
    "\n",
    "# resets the index of a Dataframe, which mens it creatse a new range index starting from 0 to len(df)-1, old indexes are dropped\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c0cd89e-fc01-4b20-9a25-8209b0a9df59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full dataset: (422419, 3)\n",
      "Train dataset: (337935, 3)\n",
      "Test dataset: (84484, 3)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"t is designed to hold and process the news data.\n",
    "    It tokenizes the news titles using DistilBertTokenizer.\n",
    "    It converts text data into numerical tensors (input IDs, attention masks) for training a deep learning model.\"\"\"\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.len = len(dataframe)\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "         # uses integer-based indexing meaning index is treated as a position in the df. 0 selects the row at the specified index, TITLE column\n",
    "        title = str(self.data.iloc[index, 0])\n",
    "        title = \" \".join(title.split())\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            title,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding='max_length',\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True\n",
    "        )\n",
    "        ids = inputs['input_ids']\n",
    "        mask = inputs['attention_mask']\n",
    "\n",
    "        return {\n",
    "            'ids': torch.tensor(ids, dtype=torch.long),\n",
    "            'mask': torch.tensor(mask, dtype=torch.long),\n",
    "            'targets': torch.tensor(self.data.iloc[index, 2], dtype=torch.long) # same thing with iloc here, 2 means we access column 3\n",
    "        }\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "\n",
    "train_size = 0.8\n",
    "train_dataset = df.sample(frac=train_size,random_state=200)\n",
    "test_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n",
    "\n",
    "train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"Full dataset: {}\".format(df.shape))\n",
    "print(\"Train dataset: {}\".format(train_dataset.shape))\n",
    "print(\"Test dataset: {}\".format(test_dataset.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f3ea0650-1a8a-4a6e-8bc4-df569cfc10af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loader <torch.utils.data.dataloader.DataLoader object at 0x72f1845f9660>\n",
      "Testing loader <torch.utils.data.dataloader.DataLoader object at 0x72f18b91cac0>\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4\n",
    "VALID_BATCH_SIZE = 2\n",
    "\n",
    "training_set = NewsDataset(train_dataset,tokenizer,MAX_LEN)\n",
    "testing_set = NewsDataset(test_dataset,tokenizer,MAX_LEN)\n",
    "\n",
    "train_parameters = {\n",
    "                    'batch_size':TRAIN_BATCH_SIZE,\n",
    "                    'shuffle':True,\n",
    "                    'num_workers':0\n",
    "                    }\n",
    "test_parameters = {\n",
    "                    'batch_size':VALID_BATCH_SIZE,\n",
    "                    'shuffle':True,\n",
    "                    'num_workers':0\n",
    "                    }\n",
    "\n",
    "\n",
    "training_loader = DataLoader(training_set, **train_parameters)\n",
    "testing_loader = DataLoader(testing_set, **test_parameters)\n",
    "print(\"Training loader {}\".format(training_loader))\n",
    "print(\"Testing loader {}\".format(testing_loader))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "89b84ce9-8a29-43cb-8387-43171678c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DistilBERTClass(torch.nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        super(DistilBERTClass,self).__init__()\n",
    "\n",
    "        self.l1 = DistilBertModel.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "        self.pre_classifier = torch.nn.Linear(768,768)\n",
    "\n",
    "        self.dropout = torch.nn.Dropout(0.3)\n",
    "\n",
    "        self.classifier = torch.nn.Linear(768,4)\n",
    "\n",
    "    def forward(self,input_ids, attention_mask):\n",
    "\n",
    "        output_1 = self.l1(input_ids=input_ids,attention_mask=attention_mask)\n",
    "\n",
    "        hidden_state = output_1[0]\n",
    "\n",
    "        pooler = hidden_state[:,0]\n",
    "\n",
    "        pooler = self.pre_classifier(pooler)\n",
    "\n",
    "        pooler = torch.nn.ReLU()(pooler)\n",
    "\n",
    "        pooler = self.dropout(pooler)\n",
    "\n",
    "        output = self.classifier(pooler)\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def calculate_accu(big_idx,targets):\n",
    "    n_correct = (big_idx==targets).sum().item()\n",
    "    '''\n",
    "    [0.88,0.1,0.33,0.7] # I love The Office 1 0 0 0 target 1 0 0 0\n",
    "    [0.99,0.04,0.5,0.77] # Friends is a great show 1 0 0 0 target 1 0 0 0\n",
    "    [0.38,0.12,0.1,0.88] # Elon Musk lands on Mars 0 0 0 1 target 0 0 0 1\n",
    "    [0.2,00.1,.7,0.55] # Breakthrough in cancer vaccine 0 0 1 0 target 0 0 1 0\n",
    "    #print(big_idx == targets) # tensor ([True, True, True, True])\n",
    "    #print(big_idx == targets).sum() # tensor(4)\n",
    "    print(big_idx == targets).sum().item() # 4\n",
    "    '''\n",
    "\n",
    "    return n_correct\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "94a439cf-e6c3-4575-9b0b-e739f027d59f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train(epoch, model, device, training_loader, optimizer, loss_function):\n",
    "    tr_loss = 0\n",
    "    n_correct = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "    model.train()\n",
    "\n",
    "    for _,data in enumerate(training_loader,0):\n",
    "        ids = data['ids'].to(device, dtype = torch.long)\n",
    "        mask = data['mask'].to(device, dtype = torch.long)\n",
    "        targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "        outputs = model(ids, mask)\n",
    "\n",
    "\n",
    "        loss = loss_function(outputs,targets)\n",
    "        tr_loss += loss.item()\n",
    "        big_val, big_idx = torch.max(outputs.data, dim = 1)\n",
    "        n_correct += calculate_accu(big_idx, targets)\n",
    "\n",
    "        nb_tr_steps +=1\n",
    "        nb_tr_examples +=targets.size(0)\n",
    "\n",
    "        if _ %5000 == 0:\n",
    "            loss_step = tr_loss/nb_tr_steps\n",
    "            accu_step = (n_correct*100)/nb_tr_examples\n",
    "            print(f\"Training loss per 5000 steps: {loss_step}\")\n",
    "            print(f\"Training Accuracy per 5000 steps: {accu_step}\")\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "\n",
    "    #print(f\"The total accuracy for epoch {epoch}: {(n_correrct*100)/nb_tr_examples}\")\n",
    "    epoch_loss = tr_loss / nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Training Loss Epoch: {epoch_loss}\")\n",
    "    print(f\"Training accuracy Epoch: {epoch_accu}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def valid(epoch, model, testing_loader, device, loss_function):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    n_correct = 0\n",
    "    tr_loss = 0\n",
    "    nb_tr_steps = 0\n",
    "    nb_tr_examples = 0\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for _, data in enumerate(testing_loader,0):\n",
    "\n",
    "            ids = data['ids'].to(device, dtype = torch.long)\n",
    "            mask = data['mask'].to(device, dtype = torch.long)\n",
    "            targets = data['targets'].to(device, dtype = torch.long)\n",
    "\n",
    "            outputs = model(ids, mask).squeeze()\n",
    "\n",
    "            loss = loss_function(outputs, targets)\n",
    "            tr_loss += loss.item()\n",
    "            big_val, big_idx = torch.max(outputs.data, dim = 1)\n",
    "            n_correct += calculate_accu(big_idx,targets)\n",
    "\n",
    "            nb_tr_steps +=1\n",
    "            nb_tr_examples += targets.size(0)\n",
    "\n",
    "            if _ % 1000 == 0:\n",
    "                loss_step = tr_loss/nb_tr_steps\n",
    "                accu_step = (n_correct*100)/nb_tr_examples\n",
    "                print(f\"Validation loss per 1000 steps: {loss_step}\")\n",
    "                print(f\"Validation accuracy per 1000 steps: {accu_step}\")\n",
    "\n",
    "    epoch_loss = tr_loss/nb_tr_steps\n",
    "    epoch_accu = (n_correct*100)/nb_tr_examples\n",
    "    print(f\"Validation loss per Epoch: {epoch_loss} at epoch {epoch}\")\n",
    "    print(f\"Validation accuracy epoch: {epoch_accu} at epoch {epoch}\")\n",
    "\n",
    "    return\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ceef22f5-1ff3-4be4-96d4-2b1a791ad9e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--epochs EPOCHS]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--valid_batch_size VALID_BATCH_SIZE]\n",
      "                             [--learning_rate LEARNING_RATE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /home/mayyi/.local/share/jupyter/runtime/kernel-87c553e3-0d32-46be-9f81-a0bf2c5bb8cb.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mayyi/.virtualenvs/ai_test/lib/python3.10/site-packages/IPython/core/interactiveshell.py:3587: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    print(\"start\")\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument(\"--epochs\",type=int,default=10)\n",
    "    parser.add_argument(\"--train_batch_size\",type=int,default=4)\n",
    "    parser.add_argument(\"--valid_batch_size\",type=int,default=2)\n",
    "    parser.add_argument(\"--learning_rate\",type=float,default=5e-5)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    args.epochs\n",
    "    args.train_batch_size\n",
    "\n",
    "\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "    model = DistilBERTClass()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    LEARNING_RATE = 1e-05\n",
    "    optimizer = torch.optim.Adam(params = model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "    loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    # Train loop\n",
    "\n",
    "    EPOCHS = 2\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        print(f\"starting epoch: {epoch}\")\n",
    "\n",
    "        train(epoch, model, device, training_loader, optimizer, loss_function)\n",
    "\n",
    "        valid(epoch, model, testing_loader, device, loss_function)\n",
    "\n",
    "\n",
    "    output_dir = os.environ['SM_MODEL_DIR']\n",
    "\n",
    "    output_model_file = os.path.join(output_dir, 'pytorch_distilbert_news.bin')\n",
    "\n",
    "    output_vocab_file = os.path.join(output_dir, 'vocab_distilbert_news.bin')\n",
    "\n",
    "    torch.save(model.state_dict(),output_model_file)\n",
    "\n",
    "    tokenizer.save_vocabulary(output_vocab_file)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "419f5094-5276-41d8-a0a2-33f62c7f74cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing: 100%|\u001b[32m███████████████████████████████████████████████████\u001b[0m| 100/100 [00:00<00:00, 48127.41it/s]\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "tqdm.pandas(desc=\"Processing\", colour=\"GREEN\")  # Enable tqdm for pandas\n",
    "dftq = pd.DataFrame({\"A\": range(100)})\n",
    "\n",
    "dftq[\"B\"] = dftq[\"A\"].progress_apply(lambda x: x * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc7a89-8fe4-4aa6-a1c1-1ceb14c0d426",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3d8a2-34a3-4a22-820a-a0c1bbd4fe23",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
